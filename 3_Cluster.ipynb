{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d21425",
   "metadata": {},
   "source": [
    "Version 1.0, 1-10-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9235a8-5221-490b-888a-24ebc8da5b88",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **3. Skript: Clustering der Stellungnahmen nach \"Bemerkung\", separat pro Artikel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de56015-fdea-4963-96e9-7cd756850b61",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Annahmen und Voraussetzungen**\n",
    "Ordner:\n",
    "- Ausgabe: Ordner \"clustering\" in Ordner \"output\" muss vorhanden sein\n",
    "\n",
    "\n",
    "xlsx-file\n",
    "- Clustering nach dem Text in der Spalte \"Bemerkung\"\n",
    "- xlsx muss eine Spalte mit Titel \"Artikel\" beinhalten, es wird pro Artikel geclustert\n",
    "\n",
    "- Die neue Spalte \"Cluster\" wird vom Skript ganz links eingefügt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26d93e-dd6f-4407-9e78-eae6f6a890ab",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Skript läuft in Azure Machine Learning Studio Empfohlene Compute-Umgebung: 16 Kerne, 64 GB RAM, 400 GB Festplatte (CPU) Kernel: Python 3.10 SDK v2**\n",
    "\n",
    "**1. Zelle: Installationen von zusätzlichen Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cdebe-7ca0-445f-9b37-ae156444f15a",
   "metadata": {
    "gather": {
     "logged": 1759138660931
    }
   },
   "outputs": [],
   "source": [
    "%pip install sentence-transformers==3.3.1 natsort\n",
    "#%pip install keras==2.12.0 #for VM\n",
    "%pip install keras\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c3318-1cce-4a49-a99f-fc8736b6c770",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad0e1189-429b-4ee9-b27f-5702b1a72a71",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**2. Zelle: Imports, define parameters, set hardcoded information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647daa40-7a0d-4360-ac80-b2935789a26c",
   "metadata": {
    "gather": {
     "logged": 1759138768980
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print (\"panda ok\")\n",
    "import matplotlib.pyplot as plt\n",
    "print (\"matplot ok\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print (\"sentence ok\")\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import natsort\n",
    "import time\n",
    "\n",
    "# Ask the user to select the Excel file\n",
    "#file_path = \"VL_Auswertung_mini_for_test.xlsx\" #since file is in top folder\n",
    "INPUT_FILE_PATH = \"/home/azureuser/cloudfiles/code/Users/matthias.rinderknecht/output.xlsx_duplicates_removed.xlsx\"\n",
    "\n",
    "# Define the folder where output documents will be saved\n",
    "OUTPUT_FOLDER_PATH = '/home/azureuser/cloudfiles/code/Users/matthias.rinderknecht/VNL/output/clustering'  # Replace with the path to the folder where output documents will be saved\n",
    "\n",
    "# LLM-Model:\n",
    "LLM_MODEL = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# Set clustering threshold\n",
    "DISTANCE_THRESHOLD_ARTICLES = 0.4\n",
    "DISTANCE_THRESHOLD_ALLG = 0.2\n",
    "\n",
    "# Set space for labels in pdf plots (0.2 is standard, increase if long organization names)\n",
    "LABEL_SPACE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e9bc6-e1eb-464b-afb6-e7c7b6cabfbd",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**3. Zelle: Hauptskript**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7998c5",
   "metadata": {
    "gather": {
     "logged": 1759138823595
    }
   },
   "outputs": [],
   "source": [
    "# to do: spacing below graphs for name of organisations\n",
    "# 20.8.2025: fixed error in cluster column insertion place \n",
    "# works in azure ML studio\n",
    "# also clusters allg bemerkungen, with distinct clustering thresholds\n",
    "# inserts sorting number (original sort of Bemerkungen) and cluster sorting number\n",
    "# WORKS\n",
    "# 27.3.: only clusters those sheets with 5 or more rows\n",
    "# 13.3.24: displays Organisation in the dendrogram instead of Feedback number\n",
    "# 13.3.24: goes through all sheets in an input file\n",
    "# Cluster 0 treated as normal cluster (only =noise in HDBSCAN)\n",
    "# Articles with just one row receive cluster number 0 (enables later on summarization by cluster label)\n",
    "#Clustering, many Articles, input .xls, output .xls + clustering report\n",
    "# works\n",
    "# caveat: dendrogram outputs only for those articles that to have \"real\" clusters, meaning more than one row\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load all sheets from the selected Excel file\n",
    "xls = pd.ExcelFile(INPUT_FILE_PATH)\n",
    "\n",
    "# Create a writer object for the output Excel file\n",
    "output_filename = f'clustered_data_threshold_{DISTANCE_THRESHOLD_ARTICLES}.xlsx'\n",
    "output_full_path = os.path.join(OUTPUT_FOLDER_PATH, output_filename)\n",
    "output_excel = pd.ExcelWriter(output_full_path, engine='openpyxl')\n",
    "\n",
    "# Iterate over each sheet in the Excel file\n",
    "for sheet_name in xls.sheet_names:\n",
    "    # Load the data from the current sheet\n",
    "    data = pd.read_excel(INPUT_FILE_PATH, sheet_name=sheet_name)\n",
    "    print (sheet_name) #DEBUG\n",
    "    \n",
    "    #add a sorting number to be able to reproduce initial sorting\n",
    "    data['Sortiernummer (nicht löschen)'] = range(1, len(data) + 1)\n",
    "    \n",
    "    # Initialize SentenceTransformer model\n",
    "    model = SentenceTransformer(LLM_MODEL)\n",
    "\n",
    "    # Check if the column names include \"Artikel\"\n",
    "    if 'Artikel' in data.columns:\n",
    "        # Create an empty DataFrame to store clustered data for all articles in the current sheet\n",
    "        all_clustered_data = pd.DataFrame()\n",
    "\n",
    "        # Replace NaN values with empty strings in the column to be clustered\n",
    "        data['Bemerkung'].fillna(\"\", inplace=True)\n",
    "        # Replace missing values in the \"Artikel\" column with \"keiner angegeben\"\n",
    "        data['Artikel'].fillna('keiner angegeben', inplace=True)\n",
    "\n",
    "        # Group data by the 'Artikel' column and sort in natural order\n",
    "        grouped_data = data.groupby('Artikel')\n",
    "        sorted_grouped_data = natsort.natsorted(grouped_data, key=lambda x: x[0]) #sorted_grouped_data becomes a list\n",
    "\n",
    "        # Create a PDF file to save all dendrograms\n",
    "        pdf_file_name = f'dendrograms_{sheet_name}_threshold_{DISTANCE_THRESHOLD_ARTICLES}.pdf'\n",
    "        pdf_output_path = os.path.join(OUTPUT_FOLDER_PATH, pdf_file_name)\n",
    "        pdf_pages = PdfPages(pdf_output_path)\n",
    "\n",
    "        # Iterate over each article group and perform clustering\n",
    "        for article_name, article_group in sorted_grouped_data:\n",
    "            # Check the number of rows for the current article\n",
    "            num_rows = len(article_group)\n",
    "            #print (num_rows) #DEBUG\n",
    "\n",
    "            if num_rows <= 1:  # Skip clustering if one or fewer rows for this article\n",
    "                # Set cluster_labels to 0 for Articles with just one row\n",
    "                article_group.insert(0, 'cluster', 0)\n",
    "                all_clustered_data = pd.concat([all_clustered_data, article_group])\n",
    "                continue\n",
    "\n",
    "            snippets = article_group['Bemerkung'].tolist()\n",
    "            snippet_embeddings = model.encode(snippets)\n",
    "\n",
    "            # Cluster the snippets using Agglomerative Clustering\n",
    "            agglomerative = AgglomerativeClustering(n_clusters=None, distance_threshold=DISTANCE_THRESHOLD_ARTICLES, linkage='complete', metric='cosine')\n",
    "            cluster_labels = agglomerative.fit_predict(snippet_embeddings)\n",
    "\n",
    "            # Count the number of clusters formed (assuming continuous cluster labels)\n",
    "            num_clusters = np.max(cluster_labels) + 1\n",
    "\n",
    "            # Add the cluster labels to the original dataframe for this article\n",
    "            article_group.insert(0, 'cluster', cluster_labels)\n",
    "\n",
    "            # Store clustered data for this article in the combined DataFrame\n",
    "            all_clustered_data = pd.concat([all_clustered_data, article_group])\n",
    "\n",
    "            # Compute the linkage matrix\n",
    "            linkage_matrix = linkage(snippet_embeddings, method='complete', metric='cosine')\n",
    "\n",
    "            # Get organization names\n",
    "            organizations = article_group['Organisation'].tolist()\n",
    "\n",
    "            # Plot dendrogram for this article\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            dendrogram(linkage_matrix, truncate_mode='level', p=num_rows - 1, leaf_rotation=90., leaf_font_size=8., color_threshold=DISTANCE_THRESHOLD_ARTICLES, labels=organizations)  # Adjust color_threshold as needed\n",
    "            plt.title(f'Dendrogram for Article: {article_name} - {num_clusters} cluster(s)')\n",
    "            plt.xlabel('Organization')\n",
    "            plt.ylabel('Distance')\n",
    "                \n",
    "            # Adjust bottom margin to increase space for x-axis label and rotated tick labels\n",
    "            plt.subplots_adjust(bottom=0.3)  # Increase the bottom space (default is usually around 0.1)\n",
    "\n",
    "            # If you want to remove the bottom spine, first get the current axis, then hide spine\n",
    "            ax = plt.gca()\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "            # Add a horizontal line for the cluster threshold\n",
    "            plt.axhline(y=DISTANCE_THRESHOLD_ARTICLES, color='r', linestyle='--')  # Adjust the y value (0.7) according to your desired threshold\n",
    "\n",
    "            # Save the current dendrogram to the PDF file\n",
    "            pdf_pages.savefig()\n",
    "\n",
    "            # Close the current figures to release memory\n",
    "            plt.close('all')\n",
    "\n",
    "            ## Plot second graph\n",
    "            # Create a new figure for the additional graph with adjusted size and layout\n",
    "            plt.figure(figsize=(7, 6))  # Adjust figure size\n",
    "            plt.subplots_adjust(bottom=0.3)  # Adjust bottom margin\n",
    "\n",
    "            # Reorder the bars as requested\n",
    "            labels = ['Total Rows', 'Number of Clusters'] + [f'Rows per Cluster {i}' for i in range(0, num_clusters)]\n",
    "\n",
    "            # ev. \n",
    "            values = [num_rows, num_clusters] + [np.sum(cluster_labels == i) for i in range(0, num_clusters)]\n",
    "\n",
    "            # Color the \"Noise Rows\" bar red and \"Number of Clusters\" bar green\n",
    "            colors = ['red' if label == 'Total Rows' else \"blue\" if label != 'Number of Clusters' else 'green' for label in labels]\n",
    "\n",
    "            # Plot the bars\n",
    "            bars = plt.bar(labels, values, color=colors)\n",
    "\n",
    "            # Display the counts on each bar\n",
    "            for bar, value in zip(bars, values):\n",
    "                plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(value), ha='center', va='bottom')\n",
    "\n",
    "            plt.title(f'Graph for Article: {article_name}')\n",
    "            plt.ylabel('Count')\n",
    "\n",
    "            # Rotate x-axis labels\n",
    "            plt.xticks(rotation=35, ha='right')\n",
    "\n",
    "            # Save the current additional graph to the PDF file\n",
    "            pdf_pages.savefig()\n",
    "\n",
    "            # Close the current figures to release memory\n",
    "            plt.close('all')\n",
    "\n",
    "        # Close the PDF file after saving all dendrograms\n",
    "        pdf_pages.close()\n",
    "\n",
    "        # Check if the 'cluster' column exists before sorting\n",
    "        # ev nach oben nehmen um Excel schon abzuschliessen, bevor PDF geschrieben wird\n",
    "        if 'cluster' in all_clustered_data.columns:\n",
    "            # Sort the clustered data within each article group by 'cluster'\n",
    "            all_clustered_data['sorting_key'] = all_clustered_data['Artikel'].apply(lambda x: natsort.natsort_keygen()(x))\n",
    "            all_clustered_data = all_clustered_data.sort_values(by=['sorting_key', 'cluster'], ascending=[True, True]).drop(columns=['sorting_key'])\n",
    "\n",
    "            # Add a new column \"Clustersortiernummer (nicht löschen)\" with serial numbers starting from 1\n",
    "            all_clustered_data['Clustersortiernummer (nicht löschen)'] = range(1, len(data) + 1)\n",
    "            \n",
    "            # Write the clustered data to the output Excel file, each sheet corresponds to the original sheet in the input file\n",
    "            all_clustered_data.to_excel(output_excel, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    else:\n",
    "        # No 'Artikel' column found, cluster using 'Bemerkungen' column\n",
    "        snippets = data['Bemerkung'].tolist()\n",
    "        snippet_embeddings = model.encode(snippets)\n",
    "\n",
    "        # Cluster the snippets using Agglomerative Clustering\n",
    "        agglomerative = AgglomerativeClustering(n_clusters=None, distance_threshold=DISTANCE_THRESHOLD_ALLG, linkage='complete', metric='cosine')\n",
    "        cluster_labels = agglomerative.fit_predict(snippet_embeddings)\n",
    "\n",
    "        # Add the cluster labels to the original dataframe\n",
    "        data.insert(0, 'cluster', cluster_labels)\n",
    "        data.sort_values(by='cluster', ascending=True, inplace=True)\n",
    "        \n",
    "        # Add a new column \"Clustersortiernummer (nicht löschen)\" with serial numbers starting from 1\n",
    "        data['Clustersortiernummer (nicht löschen)'] = range(1, len(data) + 1)\n",
    "\n",
    "        # Write the data to the output Excel file\n",
    "        data.to_excel(output_excel, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print (\"just before saving excel\")\n",
    "# Save and close the output Excel file\n",
    "output_excel.close()\n",
    "print(f\"Excel file written to: {output_full_path}\")\n",
    "\n",
    "# Record the end time and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time (s):\", time_taken)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "de"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
