{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3fc8c8",
   "metadata": {},
   "source": [
    "Version 1.0, 1-10-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc4b2df-0089-44a0-9195-a4e7b1c45945",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **2. Skript: Entfernen von Duplikaten**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e41fa3-3c65-447b-ac88-543c14d3356c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Skript imports the xlsx from script 1, and consolidates as follows:\n",
    "\n",
    "- removes all exact duplicates (same \"Bemerkung\" or same \"Bemerkung\" and \"Textvorschlag\" from same \"Organisation\", these are Stellungnahmen from the same organisation that were imported twice)\n",
    "- consolidates duplicates from different organisations into one row (same \"Bemerkung\" or same \"Bemerkung\" and \"Textvorschlag\" from different \"Organisation\", the name of all organisations having the same comments is kept and copied into one row)\n",
    "\n",
    "**Skript läuft in Azure Machine Learning Studio Empfohlene Compute-Umgebung: 16 Kerne, 64 GB RAM, 400 GB Festplatte (CPU) Kernel: Python 3.10 SDK v2**\n",
    "\n",
    "**1. Zelle: Installationen von zusätzlichen Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f90c7-01e0-439f-8174-b07501d3b444",
   "metadata": {
    "gather": {
     "logged": 1759137237569
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas==2.1.1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6993231-cfdf-44b1-8dec-4139c32f96e3",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**2. Zelle: Imports, define parameters, set hardcoded information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba78ee9d-f451-4f91-abcf-46faaa216c7e",
   "metadata": {
    "gather": {
     "logged": 1759137369583
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = \"/YOUR/INPUT/FOLDER/PATH/HERE/FILE.XLSX\"\n",
    "output_file = f'{input_file}_duplicates_removed.xlsx'\n",
    "log_path = \"duplicate_removal_log.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571de397-e284-4169-8df5-071e2bddca22",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**3. Zelle: Hauptskript**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bbbee8-013c-4fad-8710-4a1f9fca3171",
   "metadata": {
    "gather": {
     "logged": 1759141271783
    }
   },
   "outputs": [],
   "source": [
    "# works\n",
    "# removes all exact copies in organization, artikel, absatz, buchstabe, Bemerkung and Textvorschlag =exakte duplikate, doppelte importe\n",
    "# removes all exact copies within bemerkung and textvorschlag (= exakte duplikate, aber von verschiedenen Organisationen), schreibt kopierende Organisationen ein\n",
    "# writes a log file\n",
    "\n",
    "# Read all sheets from the Excel file into a dictionary of DataFrames\n",
    "all_sheets = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "output_sheets = {}\n",
    "log_lines = []\n",
    "\n",
    "for sheet_name, df in all_sheets.items():\n",
    "\n",
    "    #First remove all exact duplicates (multiple imports)\n",
    "\n",
    "    log_lines.append(f\"Sheet: {sheet_name}\")\n",
    "\n",
    "    # Base subset columns for exact duplicate removal\n",
    "    subset_cols = [\"Organisation\", \"Bemerkung\"]\n",
    "\n",
    "    if \"Textvorschlag\" in df.columns:\n",
    "        # When Textvorschlag exists, require these additional columns for duplicates to match\n",
    "        subset_cols.extend([\"Textvorschlag\", \"Artikel\", \"Absatz\", \"Buchstabe\"])\n",
    "    else:\n",
    "        # If Textvorschlag does not exist, do not include those extra columns\n",
    "        if \"Textvorschlag\" in subset_cols:\n",
    "            subset_cols.remove(\"Textvorschlag\")\n",
    "\n",
    "    duplicates_mask = df.duplicated(subset=subset_cols, keep='first')\n",
    "    exact_duplicates = df[duplicates_mask]\n",
    "    num_exact_duplicates = len(exact_duplicates)\n",
    "    log_lines.append(f\"  Exact duplicate rows removed (by columns {subset_cols}): {num_exact_duplicates}\")\n",
    "\n",
    "    if num_exact_duplicates > 0:\n",
    "        log_lines.append(f\"  Exact duplicate row indices: {list(exact_duplicates.index)}\")\n",
    "        unique_orgs = exact_duplicates[\"Organisation\"].unique()\n",
    "        log_lines.append(f\"  Organisations removed as duplicates: {list(unique_orgs)}\")\n",
    "\n",
    "    df_unique = df.drop_duplicates(subset=subset_cols, keep='first')\n",
    "\n",
    "    # Check for required columns\n",
    "    has_bemerkung = \"Bemerkung\" in df_unique.columns\n",
    "    has_textvorschlag = \"Textvorschlag\" in df_unique.columns\n",
    "    has_organisation = \"Organisation\" in df_unique.columns\n",
    "\n",
    "    if not has_bemerkung or not has_organisation:\n",
    "        log_lines.append(\"  Missing required columns for further processing. Skipping duplicate grouping.\")\n",
    "        output_sheets[sheet_name] = df_unique\n",
    "        continue\n",
    "\n",
    "    #Second, based on cleaned df from above, consolidate identical comments from different organizations    \n",
    "\n",
    "    bemerkung_dict = {}\n",
    "    # Count of groups by key, tracking number of total copies per group including original\n",
    "    group_counts = {}\n",
    "\n",
    "    for idx, row in df_unique.iterrows():\n",
    "        key = (row[\"Bemerkung\"], row[\"Textvorschlag\"]) if has_textvorschlag else (row[\"Bemerkung\"],)\n",
    "        organisation = str(row[\"Organisation\"])\n",
    "\n",
    "        if key in bemerkung_dict:\n",
    "            bemerkung_dict[key][\"Organisation\"] += \"; \" + organisation\n",
    "            bemerkung_dict[key][\"Count\"] += 1\n",
    "        else:\n",
    "            bemerkung_dict[key] = {\"Organisation\": organisation, \"Count\": 1}\n",
    "\n",
    "    # Track which keys appear more than once (duplicates removed)\n",
    "    duplicate_keys = [k for k, v in bemerkung_dict.items() if v[\"Count\"] > 1]\n",
    "    log_lines.append(f\"  Number of groups with duplicates removed: {len(duplicate_keys)}\")\n",
    "    if duplicate_keys:\n",
    "        log_lines.append(\"  Duplicate group keys (Bemerkung, Textvorschlag if present):\")\n",
    "        for k in duplicate_keys:\n",
    "            # Convert key to string and truncate to first 50 characters\n",
    "            key_str = str(k)\n",
    "            truncated_key = key_str[:50] + (\"...\" if len(key_str) > 50 else \"\")\n",
    "            log_lines.append(f\"    {truncated_key} - total copies: {bemerkung_dict[k]['Count']}\")\n",
    "\n",
    "    merged_data = []\n",
    "    for idx, row in df_unique.iterrows():\n",
    "        key = (row[\"Bemerkung\"], row[\"Textvorschlag\"]) if has_textvorschlag else (row[\"Bemerkung\"],)\n",
    "        if key in bemerkung_dict:\n",
    "            new_row = row.copy()\n",
    "            new_row[\"Organisation\"] = bemerkung_dict[key][\"Organisation\"]\n",
    "            new_row[\"Anzahl Kopien\"] = bemerkung_dict[key][\"Count\"]\n",
    "            merged_data.append(new_row)\n",
    "            del bemerkung_dict[key]\n",
    "\n",
    "    merged_df = pd.DataFrame(merged_data)\n",
    "\n",
    "    # Insert \"Anzahl Duplikate\" after \"Organisation\"\n",
    "    cols = list(merged_df.columns)\n",
    "    if \"Anzahl Kopien\" in cols and \"Organisation\" in cols:\n",
    "        org_idx = cols.index(\"Organisation\")\n",
    "        cols.insert(org_idx + 1, cols.pop(cols.index(\"Anzahl Kopien\")))\n",
    "        merged_df = merged_df[cols]\n",
    "\n",
    "    output_sheets[sheet_name] = merged_df\n",
    "    log_lines.append(\"\")  # Empty line for sheet separation\n",
    "\n",
    "# Write results to a new Excel file preserving sheet structure\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    for sheet, data in output_sheets.items():\n",
    "        data.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "# Write log to file\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(log_lines))\n",
    "\n",
    "print(f\"Processing done. Output saved to '{output_file}'.\")\n",
    "print(f\"Duplicate removal log saved to '{log_path}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "de"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
